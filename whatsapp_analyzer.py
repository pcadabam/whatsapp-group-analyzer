import re
import pandas as pd
from datetime import datetime
from collections import Counter, defaultdict
import matplotlib.pyplot as plt
import seaborn as sns
from textblob import TextBlob
from wordcloud import WordCloud
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import LatentDirichletAllocation
import warnings
warnings.filterwarnings('ignore')

class WhatsAppAnalyzer:
    def __init__(self, file_path):
        self.file_path = file_path
        self.messages = []
        self.df = None
        
    def parse_chat(self):
        """Parse WhatsApp chat export"""
        pattern = r'\[(\d{2}/\d{2}/\d{2}),\s(\d{1,2}:\d{2}:\d{2}\s[AP]M)\]\s([^:]+):\s(.+)'
        
        with open(self.file_path, 'r', encoding='utf-8') as file:
            lines = file.readlines()
        
        current_msg = None
        for line in lines:
            match = re.match(pattern, line)
            if match:
                if current_msg:
                    self.messages.append(current_msg)
                
                date_str, time_str, sender, message = match.groups()
                timestamp_str = f"{date_str}, {time_str}"
                timestamp = datetime.strptime(timestamp_str, '%d/%m/%y, %I:%M:%S %p')
                
                msg_text = message.strip()
                if 'omitted' in msg_text:
                    msg_text = '<Media omitted>'
                
                current_msg = {
                    'timestamp': timestamp,
                    'sender': sender.strip().replace('~', '').strip(),
                    'message': msg_text,
                    'hour': timestamp.hour,
                    'day_of_week': timestamp.strftime('%A'),
                    'date': timestamp.date()
                }
            elif current_msg and line.strip():
                current_msg['message'] += ' ' + line.strip()
        
        if current_msg:
            self.messages.append(current_msg)
        
        self.df = pd.DataFrame(self.messages)
        
        if len(self.messages) > 0:
            print(f"ğŸ“± Parsed {len(self.messages)} messages from {len(self.df['sender'].unique())} participants")
        else:
            print("âš ï¸ No messages found. Please check the chat file format.")
        return self.df
    
    def analyze_activity(self):
        """Analyze participant activity"""
        print("\nğŸ† PARTICIPANT LEADERBOARD")
        print("=" * 50)
        
        activity_stats = []
        for sender in self.df['sender'].unique():
            sender_msgs = self.df[self.df['sender'] == sender]
            total_msgs = len(sender_msgs)
            avg_length = sender_msgs['message'].str.len().mean()
            
            activity_stats.append({
                'name': sender,
                'total_messages': total_msgs,
                'avg_message_length': avg_length,
                'most_active_hour': sender_msgs.groupby('hour').size().idxmax(),
                'favorite_day': sender_msgs.groupby('day_of_week').size().idxmax()
            })
        
        activity_df = pd.DataFrame(activity_stats).sort_values('total_messages', ascending=False)
        
        print("\nğŸ—£ï¸ TOP CHATTERS:")
        for i, row in activity_df.head(5).iterrows():
            emoji = ['ğŸ¥‡', 'ğŸ¥ˆ', 'ğŸ¥‰', '4ï¸âƒ£', '5ï¸âƒ£'][min(i, 4)]
            print(f"{emoji} {row['name']}: {row['total_messages']} messages")
            print(f"   â†’ Avg length: {row['avg_message_length']:.0f} chars")
            print(f"   â†’ Peak hour: {row['most_active_hour']}:00")
            print(f"   â†’ Favorite day: {row['favorite_day']}")
        
        print("\nğŸ¤ QUIETEST MEMBERS:")
        for _, row in activity_df.tail(3).iterrows():
            print(f"ğŸ˜´ {row['name']}: {row['total_messages']} messages")
        
        return activity_df
    
    def analyze_sentiment(self):
        """Analyze sentiment of messages"""
        print("\nğŸ˜Š SENTIMENT ANALYSIS")
        print("=" * 50)
        
        sentiments = []
        for _, row in self.df.iterrows():
            if not row['message'].startswith('<Media omitted>'):
                blob = TextBlob(row['message'])
                sentiments.append({
                    'sender': row['sender'],
                    'message': row['message'],
                    'polarity': blob.sentiment.polarity,
                    'subjectivity': blob.sentiment.subjectivity
                })
        
        sentiment_df = pd.DataFrame(sentiments)
        
        sender_sentiment = sentiment_df.groupby('sender').agg({
            'polarity': 'mean',
            'subjectivity': 'mean'
        }).round(3)
        
        print("\nğŸ˜„ MOST POSITIVE PEOPLE:")
        top_positive = sender_sentiment.nlargest(3, 'polarity')
        for sender, row in top_positive.iterrows():
            emoji = 'ğŸ˜„' if row['polarity'] > 0.3 else 'ğŸ™‚'
            print(f"{emoji} {sender}: {row['polarity']:.3f} positivity score")
        
        print("\nğŸ˜” MOST NEGATIVE/NEUTRAL PEOPLE:")
        top_negative = sender_sentiment.nsmallest(3, 'polarity')
        for sender, row in top_negative.iterrows():
            emoji = 'ğŸ˜”' if row['polarity'] < -0.1 else 'ğŸ˜'
            print(f"{emoji} {sender}: {row['polarity']:.3f} positivity score")
        
        print("\nğŸ­ MOST DRAMATIC (subjective):")
        top_subjective = sender_sentiment.nlargest(3, 'subjectivity')
        for sender, row in top_subjective.iterrows():
            print(f"ğŸ­ {sender}: {row['subjectivity']:.3f} subjectivity score")
        
        return sentiment_df
    
    def find_hot_topics(self):
        """Find hot topics using TF-IDF and topic modeling"""
        print("\nğŸ”¥ HOT TOPICS")
        print("=" * 50)
        
        messages = [msg for msg in self.df['message'] if not msg.startswith('<Media omitted>')]
        
        vectorizer = TfidfVectorizer(
            max_features=50,
            stop_words='english',
            ngram_range=(1, 2),
            min_df=3
        )
        
        tfidf_matrix = vectorizer.fit_transform(messages)
        feature_names = vectorizer.get_feature_names_out()
        
        lda = LatentDirichletAllocation(n_components=5, random_state=42)
        lda.fit(tfidf_matrix)
        
        print("\nğŸ“Š TOP 5 DISCUSSION THEMES:")
        for idx, topic in enumerate(lda.components_):
            top_words_idx = topic.argsort()[-10:][::-1]
            top_words = [feature_names[i] for i in top_words_idx]
            print(f"\nğŸ·ï¸ Theme {idx + 1}:")
            print(f"   {', '.join(top_words[:5])}")
        
        all_words = ' '.join(messages).lower()
        word_freq = Counter(all_words.split())
        
        common_topics = [
            'modi', 'bjp', 'congress', 'trump', 'usa', 'china', 'pakistan', 'election', 'politics',
            'stock', 'crypto', 'bitcoin', 'gold', 'investment', 'nifty', 'sensex', 'market',
            'bangalore', 'mumbai', 'delhi', 'chennai', 'hyderabad', 'karnataka', 'maharashtra',
            'work', 'job', 'salary', 'office', 'meeting', 'wfh', 'tech', 'software',
            'movie', 'bollywood', 'cricket', 'ipl', 'kohli', 'dhoni', 'netflix',
            'food', 'biryani', 'restaurant', 'zomato', 'swiggy', 'lunch', 'dinner',
            'travel', 'vacation', 'goa', 'kerala', 'flight', 'uber', 'ola',
            'house', 'rent', 'flat', 'traffic', 'metro', 'weather', 'rain', 'monsoon',
            'festival', 'diwali', 'holi', 'wedding', 'family', 'friends', 'party', 'weekend',
            'covid', 'vaccine', 'lockdown', 'news', 'plan', 'tomorrow', 'tonight'
        ]
        
        print("\nğŸ—£ï¸ MOST TALKED ABOUT:")
        topic_counts = {}
        import re
        for topic in common_topics:
            # Use word boundary matching for short topics to avoid false positives
            if len(topic) <= 3 and ' ' not in topic:
                # For short single words, use word boundaries
                pattern = r'\b' + re.escape(topic.lower()) + r'\b'
                count = sum(1 for msg in messages if re.search(pattern, msg.lower()))
            else:
                # For longer words and phrases, use substring matching
                count = sum(1 for msg in messages if topic.lower() in msg.lower())
            if count > 0:
                topic_counts[topic] = count
        
        for topic, count in sorted(topic_counts.items(), key=lambda x: x[1], reverse=True)[:5]:
            print(f"   â€¢ {topic.capitalize()}: mentioned {count} times")
        
        return feature_names, lda
    
    def find_polarizing_topics(self):
        """Find topics with most varied sentiment"""
        print("\nâš¡ POLARIZING TOPICS")
        print("=" * 50)
        
        topic_sentiments = defaultdict(list)
        topics_to_check = ['trump', 'biden', 'usa', 'china', 'pakistan', 'election', 'politics', 'modi', 
                          'bjp', 'congress', 'crypto', 'bitcoin', 'stock', 'market', 'investment', 'gold',
                          'movie', 'bollywood', 'cricket', 'kohli', 'ipl', 'work', 'salary', 'job',
                          'bangalore', 'mumbai', 'delhi', 'traffic', 'rent', 'food', 'restaurant',
                          'weekend', 'party', 'travel', 'weather', 'covid', 'vaccine', 'lockdown']
        
        for _, row in self.df.iterrows():
            msg_lower = row['message'].lower()
            for topic in topics_to_check:
                if topic in msg_lower and not row['message'].startswith('<Media omitted>'):
                    blob = TextBlob(row['message'])
                    topic_sentiments[topic].append(blob.sentiment.polarity)
        
        polarizing = []
        for topic, sentiments in topic_sentiments.items():
            if len(sentiments) > 2:
                std_dev = np.std(sentiments)
                polarizing.append((topic, std_dev, len(sentiments)))
        
        polarizing.sort(key=lambda x: x[1], reverse=True)
        
        print("\nğŸ”„ Topics with most varied opinions:")
        for topic, std, count in polarizing[:5]:
            print(f"   â€¢ {topic.capitalize()}: Ïƒ={std:.3f} ({count} mentions)")
        
        return polarizing
    
    def analyze_time_patterns(self):
        """Analyze time patterns"""
        print("\nâ° TIME PATTERNS")
        print("=" * 50)
        
        hourly = self.df.groupby('hour').size()
        daily = self.df.groupby('day_of_week').size()
        
        peak_hour = hourly.idxmax()
        quiet_hour = hourly.idxmin()
        peak_day = daily.idxmax()
        quiet_day = daily.idxmin()
        
        print(f"\nğŸŒŸ Peak chat hour: {peak_hour}:00 ({hourly.max()} messages)")
        print(f"ğŸ˜´ Quietest hour: {quiet_hour}:00 ({hourly.min()} messages)")
        print(f"ğŸ—“ï¸ Most active day: {peak_day} ({daily.max()} messages)")
        print(f"ğŸ¤« Quietest day: {quiet_day} ({daily.min()} messages)")
        
        if peak_hour >= 22 or peak_hour <= 2:
            print("\nğŸ¦‰ You guys are night owls!")
        elif peak_hour >= 6 and peak_hour <= 9:
            print("\nğŸŒ… Early bird squad!")
        
        return hourly, daily
    
    def find_conversation_starters(self):
        """Find who starts most conversations"""
        print("\nğŸ’¬ CONVERSATION DYNAMICS")
        print("=" * 50)
        
        self.df['time_diff'] = self.df['timestamp'].diff()
        conversation_starters = []
        
        for idx, row in self.df.iterrows():
            if idx == 0 or row['time_diff'].total_seconds() > 3600:
                conversation_starters.append(row['sender'])
        
        starter_counts = Counter(conversation_starters)
        
        print("\nğŸš€ TOP CONVERSATION STARTERS:")
        for sender, count in starter_counts.most_common(3):
            print(f"   ğŸ—£ï¸ {sender}: started {count} conversations")
        
        return starter_counts
    
    def generate_fun_awards(self):
        """Generate fun awards for group members"""
        print("\nğŸ† FUN AWARDS")
        print("=" * 50)
        
        emoji_users = defaultdict(int)
        question_users = defaultdict(int)
        exclamation_users = defaultdict(int)
        media_users = defaultdict(int)
        laugh_track = defaultdict(int)
        one_word_messages = defaultdict(int)
        late_night_messages = defaultdict(int)
        typo_messages = defaultdict(int)
        voice_notes = defaultdict(int)
        
        for _, row in self.df.iterrows():
            sender = row['sender']
            msg = row['message']
            hour = row['hour']
            
            emoji_count = len(re.findall(r'[ğŸ˜€-ğŸ™ğŸŒ€-ğŸ—¿ğŸš€-ğŸ›¿ğŸ´-ğŸ¿]', msg))
            emoji_users[sender] += emoji_count
            
            # Laugh track - count various forms of laughter
            laugh_patterns = ['haha', 'hehe', 'lol', 'rofl', 'lmao', 'ğŸ˜‚', 'ğŸ¤£', 'ğŸ˜„', 'ğŸ˜†']
            for pattern in laugh_patterns:
                if pattern in msg.lower():
                    laugh_track[sender] += 1
                    break
            
            # One word wonder - messages with single word (excluding media)
            if not msg.startswith('<Media omitted>') and len(msg.split()) == 1 and len(msg) < 20:
                one_word_messages[sender] += 1
            
            # Late night texter (2 AM - 5 AM)
            if 2 <= hour < 5:
                late_night_messages[sender] += 1
            
            # Voice notes (look for audio omitted)
            if 'audio omitted' in msg.lower():
                voice_notes[sender] += 1
            
            # Basic typo detection (messages with multiple consecutive same letters)
            if re.search(r'(.)\1{2,}', msg) or re.search(r'[a-zA-Z]{15,}', msg):
                typo_messages[sender] += 1
            
            if '?' in msg:
                question_users[sender] += 1
            if '!' in msg:
                exclamation_users[sender] += 1
            if '<Media omitted>' in msg:
                media_users[sender] += 1
        
        print(f"ğŸ˜‚ Emoji Champion: {max(emoji_users, key=emoji_users.get)}")
        print(f"â“ Question Master: {max(question_users, key=question_users.get)}")
        print(f"â— Excitement King/Queen: {max(exclamation_users, key=exclamation_users.get)}")
        if media_users:
            print(f"ğŸ“¸ Media Mogul: {max(media_users, key=media_users.get)}")
        
        if laugh_track:
            print(f"ğŸ¤£ Laugh Factory: {max(laugh_track, key=laugh_track.get)} (made people laugh most)")
        
        if one_word_messages:
            print(f"ğŸ”¤ One Word Wonder: {max(one_word_messages, key=one_word_messages.get)}")
        
        if late_night_messages:
            print(f"ğŸ¦‰ Night Owl Champion: {max(late_night_messages, key=late_night_messages.get)}")
        
        if voice_notes:
            print(f"ğŸ¤ Voice Note Addict: {max(voice_notes, key=voice_notes.get)}")
        
        avg_msg_length = self.df.groupby('sender')['message'].apply(lambda x: x.str.len().mean())
        print(f"ğŸ“œ Essay Writer: {avg_msg_length.idxmax()} ({avg_msg_length.max():.0f} chars avg)")
        print(f"ğŸ”¤ Short & Sweet: {avg_msg_length.idxmin()} ({avg_msg_length.min():.0f} chars avg)")
        
        return {
            'emoji_champion': max(emoji_users, key=emoji_users.get) if emoji_users else None,
            'question_master': max(question_users, key=question_users.get) if question_users else None,
            'excitement_person': max(exclamation_users, key=exclamation_users.get) if exclamation_users else None,
            'laugh_factory': max(laugh_track, key=laugh_track.get) if laugh_track else None,
            'one_word_wonder': max(one_word_messages, key=one_word_messages.get) if one_word_messages else None,
            'night_owl': max(late_night_messages, key=late_night_messages.get) if late_night_messages else None
        }
    
    def generate_most_likely_awards(self):
        """Generate 'Most Likely To...' awards"""
        print("\nğŸ­ MOST LIKELY TO...")
        print("=" * 50)
        
        # Calculate various metrics for each user
        user_stats = {}
        for sender in self.df['sender'].unique():
            sender_msgs = self.df[self.df['sender'] == sender]
            user_stats[sender] = {
                'total_messages': len(sender_msgs),
                'avg_hour': sender_msgs['hour'].mean(),
                'weekend_ratio': len(sender_msgs[sender_msgs['day_of_week'].isin(['Saturday', 'Sunday'])]) / max(len(sender_msgs), 1),
                'night_messages': len(sender_msgs[(sender_msgs['hour'] >= 0) & (sender_msgs['hour'] < 5)]),
                'consecutive_messages': self.count_consecutive_messages(sender),
                'media_count': len(sender_msgs[sender_msgs['message'].str.contains('Media omitted')]),
                'question_ratio': len(sender_msgs[sender_msgs['message'].str.contains('?')]) / max(len(sender_msgs), 1),
                'link_count': len(sender_msgs[sender_msgs['message'].str.contains('http|www', case=False, regex=True)])
            }
        
        # Award assignments
        awards = []
        
        # Most likely to reply at 3 AM
        night_owl = max(user_stats.items(), key=lambda x: x[1]['night_messages'])
        if night_owl[1]['night_messages'] > 0:
            awards.append(f"ğŸŒ™ Reply at 3 AM: {night_owl[0]} ({night_owl[1]['night_messages']} night messages)")
        
        # Most likely to send 20 messages in a row
        spam_king = max(user_stats.items(), key=lambda x: x[1]['consecutive_messages'])
        if spam_king[1]['consecutive_messages'] > 3:
            awards.append(f"ğŸ“± Send 20 messages in a row: {spam_king[0]} (max streak: {spam_king[1]['consecutive_messages']})")
        
        # Most likely to share links/news
        if any(stats['link_count'] > 0 for stats in user_stats.values()):
            news_sharer = max(user_stats.items(), key=lambda x: x[1]['link_count'])
            awards.append(f"ğŸ“° Share breaking news: {news_sharer[0]} ({news_sharer[1]['link_count']} links shared)")
        
        # Most likely to ask questions
        curious_cat = max(user_stats.items(), key=lambda x: x[1]['question_ratio'])
        if curious_cat[1]['question_ratio'] > 0.1:
            awards.append(f"â“ Ask 'but why?': {curious_cat[0]} ({curious_cat[1]['question_ratio']*100:.0f}% questions)")
        
        # Most likely to ghost the group (lurker)
        total_messages = len(self.df)
        lurkers = [(name, stats['total_messages']) for name, stats in user_stats.items() 
                   if stats['total_messages'] < total_messages * 0.02]
        if lurkers:
            biggest_lurker = min(lurkers, key=lambda x: x[1])
            awards.append(f"ğŸ‘» Ghost the group: {biggest_lurker[0]} (only {biggest_lurker[1]} messages)")
        
        for award in awards:
            print(f"   â€¢ {award}")
        
        return awards
    
    def count_consecutive_messages(self, sender):
        """Count maximum consecutive messages by a sender"""
        max_consecutive = 0
        current_consecutive = 0
        
        for _, row in self.df.iterrows():
            if row['sender'] == sender:
                current_consecutive += 1
                max_consecutive = max(max_consecutive, current_consecutive)
            else:
                current_consecutive = 0
        
        return max_consecutive
    
    def detect_lurkers(self):
        """Detect lurkers in the group"""
        print("\nğŸ‘€ LURKER ANALYSIS")
        print("=" * 50)
        
        total_messages = len(self.df)
        total_days = (self.df['timestamp'].max() - self.df['timestamp'].min()).days
        
        lurker_scores = []
        for sender in self.df['sender'].unique():
            sender_msgs = self.df[self.df['sender'] == sender]
            message_count = len(sender_msgs)
            participation_rate = message_count / total_messages
            
            if participation_rate < 0.05:  # Less than 5% of total messages
                msgs_per_day = message_count / max(total_days, 1)
                lurker_scores.append({
                    'name': sender,
                    'messages': message_count,
                    'percentage': participation_rate * 100,
                    'msgs_per_day': msgs_per_day
                })
        
        if lurker_scores:
            lurker_scores.sort(key=lambda x: x['messages'])
            print("ğŸ¤« Silent watchers:")
            for lurker in lurker_scores[:3]:
                print(f"   â€¢ {lurker['name']}: {lurker['messages']} messages ({lurker['percentage']:.1f}% participation)")
                if lurker['msgs_per_day'] < 0.1:
                    print(f"     â†’ Appears once every {1/max(lurker['msgs_per_day'], 0.001):.0f} days")
        else:
            print("   Everyone actively participates! No lurkers detected.")
        
        return lurker_scores
    
    def create_visualizations(self):
        """Create fun visualizations"""
        plt.style.use('seaborn-v0_8-darkgrid')
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        
        activity_by_sender = self.df['sender'].value_counts().head(10)
        axes[0, 0].barh(activity_by_sender.index, activity_by_sender.values, color='skyblue')
        axes[0, 0].set_xlabel('Number of Messages')
        axes[0, 0].set_title('ğŸ† Top 10 Most Active Members', fontsize=14, fontweight='bold')
        axes[0, 0].invert_yaxis()
        
        hourly_activity = self.df.groupby('hour').size()
        axes[0, 1].plot(hourly_activity.index, hourly_activity.values, marker='o', color='coral', linewidth=2)
        axes[0, 1].set_xlabel('Hour of Day')
        axes[0, 1].set_ylabel('Number of Messages')
        axes[0, 1].set_title('â° Chat Activity Throughout the Day', fontsize=14, fontweight='bold')
        axes[0, 1].grid(True, alpha=0.3)
        
        day_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']
        daily_activity = self.df.groupby('day_of_week').size().reindex(day_order)
        axes[1, 0].bar(daily_activity.index, daily_activity.values, color='lightgreen')
        axes[1, 0].set_xlabel('Day of Week')
        axes[1, 0].set_ylabel('Number of Messages')
        axes[1, 0].set_title('ğŸ“… Weekly Activity Pattern', fontsize=14, fontweight='bold')
        axes[1, 0].tick_params(axis='x', rotation=45)
        
        messages_text = ' '.join([msg for msg in self.df['message'] if not msg.startswith('<Media omitted>')])
        wordcloud = WordCloud(width=800, height=400, background_color='white', 
                             colormap='viridis', max_words=50).generate(messages_text)
        axes[1, 1].imshow(wordcloud, interpolation='bilinear')
        axes[1, 1].set_title('â˜ï¸ Most Common Words', fontsize=14, fontweight='bold')
        axes[1, 1].axis('off')
        
        plt.tight_layout()
        plt.savefig('/Users/pcadabam/Downloads/buddies/whatsapp_analysis.png', dpi=150, bbox_inches='tight')
        print("\nğŸ“Š Visualizations saved as 'whatsapp_analysis.png'")
        
        return fig
    
    def generate_summary_report(self):
        """Generate a fun summary report"""
        print("\n" + "=" * 60)
        print("ğŸ‰ APARTMENT GROUP CHAT PERSONALITY REPORT ğŸ‰")
        print("=" * 60)
        
        total_days = (self.df['timestamp'].max() - self.df['timestamp'].min()).days
        msgs_per_day = len(self.df) / max(total_days, 1)
        
        print(f"\nğŸ“ˆ QUICK STATS:")
        print(f"   â€¢ Total messages: {len(self.df):,}")
        print(f"   â€¢ Active members: {len(self.df['sender'].unique())}")
        print(f"   â€¢ Time period: {total_days} days")
        print(f"   â€¢ Average messages per day: {msgs_per_day:.1f}")
        
        if msgs_per_day > 100:
            print("\nğŸ”¥ This group is ON FIRE! You guys never stop talking!")
        elif msgs_per_day > 50:
            print("\nğŸ’¬ Pretty chatty group! Good vibes all around!")
        else:
            print("\nğŸ˜Œ Nice and chill group dynamics!")

def main():
    print("ğŸŠ WHATSAPP GROUP CHAT ANALYZER ğŸŠ")
    print("=" * 60)
    
    analyzer = WhatsAppAnalyzer('/Users/pcadabam/Downloads/buddies/_chat.txt')
    
    analyzer.parse_chat()
    
    analyzer.generate_summary_report()
    
    activity_df = analyzer.analyze_activity()
    
    sentiment_df = analyzer.analyze_sentiment()
    
    features, lda = analyzer.find_hot_topics()
    
    polarizing = analyzer.find_polarizing_topics()
    
    hourly, daily = analyzer.analyze_time_patterns()
    
    starters = analyzer.find_conversation_starters()
    
    awards = analyzer.generate_fun_awards()
    
    most_likely = analyzer.generate_most_likely_awards()
    
    lurkers = analyzer.detect_lurkers()
    
    fig = analyzer.create_visualizations()
    
    print("\n" + "=" * 60)
    print("ğŸ­ Analysis Complete! Share these fun insights with your group!")
    print("=" * 60)

if __name__ == "__main__":
    main()